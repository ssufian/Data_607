---
output:
  html_document: default
  pdf_document: default
---

title: "Data 607 Fall 2019 - Final Project: Medical care provider fraud detection tool"


author: "Sufian"

date: "11/14/2019"

output: html_document


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Project Statement

--------------------------------------------------------------------------------

\clearpage

The purpose of this project is two-fold:  

1) To predict the type of medical providers based on the type and number of procedures performed:

- The claim is that if the prediction is different than the actual provide type, then possible fraud

may have occured and a flag is thrown up for further investigation.  

## Why is this Important?

As an example: If a provider claim to be general practice but the model predicts it was, a cardiologist

provider, then possible medical fraud could've occured because procedures performed were not

consistent with actual provider type.  The reason this could result in a fraud is because providers

might then up-charged medicare for these procedures or worse, made multiple lower-charges

fraudelently.  Either of which costs tax payers money and a financial drain to the entire Medicare

system as a whole over the long run.

## The prediction hypothesis is as follows (2-tail test):

Ho:  there is no difference between actual and predicted provider types

Ha:  there is a difference (not equal to) between actual and predicted provider types

--------------------------------------------------------------------------------

\clearpage

2)  This extra 2nd step is for us to gauge if the Office of Inspector General's (OIG) exclusion list

were effective in catching these "bad doctors" that have commited frauds in the past.  This extra

after-the-fact exercise is simply to check the predictive power & accuracy of the tool.  

--------------------------------------------------------------------------------

\clearpage

## Data Sources:

1)  Data.CMS.gov:  Medicare Provider Utilization and Payment Data (2017):

https://data.cms.gov/Medicare-Physician-Supplier/Medicare-Provider-Utilization-and-Payment-Data-Phy/fs4p-t5eq/data

2)  US department of Health and Human services: Office of Inspector General (OIG), Exclusion Program

https://oig.hhs.gov/exclusions/authorities.asp


--------------------------------------------------------------------------------

\clearpage


## The Data Sets 

## Dataset No. 1

1) The Medicare Provide Utilization & Payment Data Provides the majority of the data for this project.  

It is a 9847443 X 26 dataframe.  Breifly, the columns are information regarding the medical providers, 

locations of practices, National provider ID, Procedure codes, Text descriptions of procedures (HCPCS),

standard, actual procedural codes or HCPCS codes, average medicare charges and its related average

charges for the procedure types just to name a few.

2)  Each row, contains medical procedures provided by the provider.  So it can contain multiple

procedure types for the same physician.  

## Dataset No. 2

1)  US department of Health and Human services' exclusion list.  This database is run by the Office

of Inspector General (OIG). This data set is also called the exclusion list. Its where the "bad doctor"

with a fraudlent past is listed along with the types of fraud they have commited.  

--------------------------------------------------------------------------------

\clearpage

## Exploratory Data Exercise

- Data Staging & transformation:  Making the database from long to wide by hot-encoding the HCPCS Code

column. This is the column that describe the medical procedure, i.e., Predictor variables (features)

- Data Cleansing (Removing unwanted columns from the databases)

## Statistical Analysis

- Distribution of medical providers, medical procedures especially in the behavior such as

any extreme outliers, skewness and other typical statistical checks to understand the data

- Check for correleational relationships between medical providers and medical procedures group_by:

(a) Geographic areas - Florida only

(b) medical practice types: groups = provider types

## Statistical Results 

(d) Chi Square test for the Hypothesis testing:  Association/Independence testing
 
(e) Correlation Matrix (This might be challenging to visualize given the high no. of variables)

## Predictive Analystics

- Accuracy is done via K-fold cross validation repeated with folds = 5 and possible stratified to

improve accuracy level if time afford.  

- Split train/test data sets on down sample sized (This is key as the actual data size is too big for

our academic purpose)

- prediction package:  multinomial Naive bayes and Random Forest 

- Prediction accuracy: F1 score:  precision & recall & Sensitivity

--------------------------------------------------------------------------------

\clearpage

## Process Work Flow 

![Fig 1: Data Work Flow](https://www.dropbox.com/s/l63jqrli07lt389/Process_Flow.JPG?dl=1/Process_Flow.JPG)

```{r,message=FALSE, warning=FALSE}
library(dplyr)
library("tidyverse")
require(data.table)

library(httr)
library("DT")
library(dplyr)
library(plotrix)

library(DBI)
library(rstudioapi)
require (ggplot2)
library(DBI)
library(pastecs)
library(ggpubr)
library(tidyr)
library(RColorBrewer)
library(psych)
library(formattable)
library(hablar)
library(varhandle)
require(reshape2)
require(scales)
library(ggcorrplot)
require(GGally)
library(e1071)
library(caret)
library(normalr)
library(nortest)
library(broom) 
library(data.table)
require(ggthemes)
library(tidyr)
library(devtools)
library(readr)
library(randomForest)
```

## The Medicare Data base was too big, therefore it was stored in public dropbox 

- This project was based only on 2017 data


```{r}
#Reading in the Medicare database from link provided by Dropbox

# NOte: Only 2017 data was utilized


download.file("https://www.dropbox.com/s/jpbw0gbcknunk6x/Medicare_Provider_Utilization_and_Payment_Data__Physician_and_Other_Supplier_PUF_CY2017.csv?dl=1","Medicare_Provider_Utilization_and_Payment_Data__Physician_and_Other_Supplier_PUF_CY2017.csv")


Dat <- read.csv("Medicare_Provider_Utilization_and_Payment_Data__Physician_and_Other_Supplier_PUF_CY2017.csv", stringsAsFactors = F,header=T)

x <- Dat %>% as_tibble()

#Peeking at the Medicare data


head(x,n=3) #9847443 X 26 dataframe

```

# Reading & Loading OIG's exclusion data base where physicians have a fraudelent past 

```{r}

url <- 'https://raw.githubusercontent.com/ssufian/Data_607/master/ELIE.csv'

df <- read.csv(file = url ,sep = ",", na.strings = c("NA", " ", ""), strip.white = TRUE, stringsAsFactors = F,header=T)

head(df,n=5)

```

--------------------------------------------------------------------------------

\clearpage

## Data cleansing, subsetting, transformation, renaming columns of the 2 databases etc

- Medicare Database with Medicare Only Provider participants

--------------------------------------------------------------------------------

\clearpage


```{r}
# Subsetting the Medicare Database to only get the relevant columns

x1 <- x %>%
  select(National.Provider.Identifier, Gender.of.the.Provider, State.Code.of.the.Provider,Provider.Type,Medicare.Participation.Indicator,HCPCS.Code,HCPCS.Description,HCPCS.Drug.Indicator,Number.of.Services,Average.Medicare.Allowed.Amount,Average.Submitted.Charge.Amount,Average.Medicare.Standardized.Amount)

head(x1,n=3)

# rename col names
x2 <- rename(x1, NPI = National.Provider.Identifier
,Gender = Gender.of.the.Provider,State =State.Code.of.the.Provider,
Provider_Type =Provider.Type,Part_Indicator=Medicare.Participation.Indicator,HCPCS_Code=HCPCS.Code,HCPCS_Desc=HCPCS.Description,Drug_Indicator = HCPCS.Drug.Indicator,
No_Services=Number.of.Services,Avg_Medi_Allowed = Average.Medicare.Allowed.Amount,Avg_Charge =Average.Submitted.Charge.Amount,Avg_Std_Amount=Average.Medicare.Standardized.Amount)

head(x2,n=3)

# Taking only Providers that are Medicare participants
x2 <- x2 %>% 
  filter(Part_Indicator=="Y")

```

## Now OIG's Exclusion Database for the "bad doctors"

```{r}
# Subsetting OIG's Exclusion Database to only get the relevant columns

df1 <- df %>% 
   select(BUSNAME,GENERAL,SPECIALTY,NPI,STATE,EXCLTYPE,EXCLDATE,REINDATE,WAIVERDATE)
# Renaming columns in the Medicare Data base to sync up with the OIG's exclusion database
head(df1,n=3)
```

## Finding the statistics of Medicare Database to see how many unique instances in the country

```{r}
# to get how many physicians in the entire 2017 database; country

x3 <- x2 %>% 
  distinct(NPI)

dim(x3)
# to get no. of providers types or specialities
x4 <- x2 %>% 
  distinct(Provider_Type)

dim(x4)

# to get no. of procedures types  (HCPCS codes)
x5 <- x2 %>% 
  distinct(HCPCS_Code)

dim(x5)

```

## Observation 1a:

The entire 2017 Medicare database (Across the country) has the following statistic:

- Number of Physicians = 1,031,978

- Number of Procedures = 6,048

- Number of Provider types = 94

## Filtering Medicare Database only on the state of Florida; and see how many unique instances

- To see if the analysis is more "digestable" by filtering on Florida; demonstrative purposes only

```{r}
# Florida only database
x_fl <- x2 %>% 
  filter (State == 'FL')

head(x_fl, n=2)

```

```{r}
# to get how many physicians in the Floridian database only
x6 <- x_fl %>% 
  distinct(NPI)

dim(x6)
# to get no. of providers types or specialities
x7 <- x_fl %>% 
  distinct(Provider_Type)

dim(x7)

# to get no. of procedures types  (HCPCS codes)
x8 <- x_fl %>% 
  distinct(HCPCS_Code)

dim(x8)
```

## Observation 1b:

The entire 2017 Medicare database (Florida only data) has the following statistic:

- Number of Physicians = 63,316

- Number of Procedures = 4,030

- Number of Provider types = 88

--------------------------------------------------------------------------------

\clearpage

## Statistical Analysis of the Florida database

-To see which provider types & procedures were most common place in Florida for 2017 and try to get a 

feel for its distributions.

```{r,fig.width=13, fig.height=14}

x_fl_Type <- x_fl %>% 
  group_by(Provider_Type) %>% 
  tally()
x_fl_Type  <- rename(x_fl_Type , Freq_Type = n )
p1 <- ggplot(data=x_fl_Type , aes(x=Provider_Type, y=Freq_Type )) +ggtitle("Fig 2: Distribution of Provider Types in 2017")+
geom_bar(stat="identity")+ coord_flip()+theme_economist()

p1

```

## Subsetting & renaming columns in preparation of one-hot encoding

```{r}

# subset even futhter 
x_fl_onehot <- x_fl %>% 
  select(NPI,Provider_Type,HCPCS_Code,No_Services) 


#spread long to wide format - hot encoding Pivoting was not use because I could not get the library installed

#x_fl_wider <- x_fl_onehot %>% 
 # pivot_wider(names_from = HCPCS_Code, values_from = No_Services)

head(x_fl_onehot, n=2)

```

## Using dcast instead to make from long to wide format of filtered (Florida only) Medicare database

```{r}
#spread long to wide format - hot encoding

# Keeping NPI column
x_onehot2 <- dcast(as.data.table(x_fl_onehot), NPI+Provider_Type~HCPCS_Code, value.var="No_Services", fun.aggregate=sum, fill=0) %>%
  as_tibble() %>%
  mutate_at(vars(-NPI,-Provider_Type), as.factor)
 
head(x_onehot2)

```


## The above data set with only State of Florida provider types resulted in=> 63316 x 4031 matrix!

## Because even after filtering only on Florida, the dataset is still too large for my computer:

- I decided to use only the top 5 provider types for the rest of the project to keep it manageable

- Lets find out which Provider types are the 5 most popular, and use those for the project

```{r}
#use the database before I hot encode (x_fl_onehot) to check frequencies of top 05 provider types
x_fl_wider_5 <- x_fl_Type%>% 
  top_n(5,Freq_Type)

(x_fl_wider_5 )

```

## Plot of the top 5 provider types in state of Florida

```{r,fig.width=12, fig.height=7}
p2 <- ggplot(data=x_fl_wider_5  , aes(x=Provider_Type, y=Freq_Type,fill = Provider_Type)) +ggtitle("Figure 3: Top 5 Florida Provider Types in 2017")+ 
geom_bar(stat="identity")+ coord_flip()+theme_economist()

p2

```

## Subsetting only top providers in Florida to get to a manageable size 

```{r}
# This was to check for top 20 floridian provider types
target20 <- c('Anesthesiology','Cardiology','Certified Registered Nurse Anesthetist (CRNA)','Dermatology','Diagnostic Radiology','Emergency Medicine','Family Practice','Gastroenterology','General Surgery','Hematology-Oncology','Internal Medicine','Neurology','Nurse Practitioner','Ophthalmology','Orthopedic Surgery','Physical Therapist in Private Practice','Physician Assistant','Podiatry','Pulmonary Disease','Urology')

# This was to check for top 10 floridian provider types
target10 <- c('Cardiology','Dermatology','Diagnostic Radiology','Family Practice','Hematology-Oncology','Internal Medicine','Nurse Practitioner','Ophthalmology','Orthopedic Surgery','Physician Assistant')

# This was to check for top 5 floridian provider types
target5 <- c('Cardiology','Diagnostic Radiology','Family Practice','Internal Medicine','Nurse Practitioner')

 data5 <- x_fl_onehot %>% filter(Provider_Type%in% target5) 

 head(data5,n=3)
 
```

## Check again the size of database

```{r}
# to get how many physicians in the top 5 Floridian database only
x9 <-  data5%>% 
  distinct(NPI)

dim(x9)
# to get no. of top 5 providers types or specialities
x10 <-  data5%>% 
  distinct(Provider_Type)

dim(x10)

# to get no. of top 5 procedures types (HCPCS codes) 
X11 <- data5%>% select(HCPCS_Code)

n_distinct(X11)

```

## Observation 1c:

The entire 2017 Medicare database (Florida's Top 5 only provider types) has the following statistic:

- Number of Physicians = 20,445

- Number of Procedures = 1,853

- Number of Provider types = 5

```{r}

x_onehot3 <- dcast(as.data.table(data5), NPI+Provider_Type~HCPCS_Code, value.var="No_Services", fun.aggregate=sum, fill=0) %>%
  as_tibble() %>%
  mutate_at(vars(-NPI,-Provider_Type), as.factor)
 
head(x_onehot3,n=3)

x_fl_widerfinal <- x_onehot3 %>% 
  select(-c(NPI))

head(x_fl_widerfinal,n=3)

```

## Naive Bayes will be the classifier used in this study to predict Provider Types based on Procedures

-  This is a multi-class problem with multiple response variables: Number of Provider types = 5

For example:

Classes (Target) : Cardiology,Diagnostic Radiology,Family Practice,Internal Medicine,Nurse Practitioner

-  The number of predictors: Number of Procedures = 1,853.  Let's begin!

## Working on a Really, Really small data set now - for demonstrative purpose only

```{r}
#Building a model
#split data into training and test data sets; 70-30 split

indxTrain <- createDataPartition(x_fl_widerfinal$Provider_Type,p = 0.70,list = FALSE)
training <- x_fl_widerfinal[indxTrain,]
testing <- x_fl_widerfinal[-indxTrain,]

#Check dimensions of the split: From the actual datasets all the way to the train & test datasets

prop.table(table(x_fl_widerfinal$Provider_Type)) * 100

prop.table(table(training$Provider_Type)) * 100

prop.table(table(testing$Provider_Type)) * 100

## check the balance of the classes in abs number terms
print(table(x_fl_widerfinal$Provider_Type))
```

## Observation 2:

As shown above from the subsetted datasets all the way to the testing data sets, The classes were not

too imbalanced:

- Cardiology = 1,488

- Diagnostic Radiology = 1,742

- Family Practice = 4,831

- Internal Medicine = 6,297

- Nurse Practitioner = 6,087

--------------------------------------------------------------------------------

\clearpage

## Create naive bayes model with Cross Validation, n folds=5

```{r}
#create objects x which holds the predictor variables and y which holds the response variables
x_nb = training[,-1]
y_nb = as.factor(training$Provider_Type)

# add smoothing function; Laplace
model = train(x_nb,y_nb,'nb',trControl=trainControl(method='cv',number=5),laplace = 0.1,na.action = na.pass)

 model #first naive bayes results

```

## Model Evaluation

```{r}
#Predict testing set
Predict <- predict(model,newdata = testing )

table(Predict, testing$Provider_Type,dnn=c("Prediction","Actual"))
```

## Confusion Matrix

```{r}
#Get the confusion matrix to see accuracy value and other parameter values
 
cm <- confusionMatrix(data=Predict, reference=as.factor(testing$Provider_Type))

cm
```

## F1 Score

```{r}

# extract F1 scores for all classes
mylist <- cm[["byClass"]][,"F1"]

byClass <- c('Cardiology', 'Diagnostic Radiology',' Family Practice','Internal Medicine','Nurse Practitioner')

mylist <- tibble(mylist)

my_F1_score <- mylist %>% 
  mutate(byClass= byClass, F1_Score = as.factor(percent(round(mylist,digits=2)))) %>% select(-c(mylist)) %>% arrange(desc(F1_Score))
my_F1_score

```

--------------------------------------------------------------------------------

\clearpage


## Observation 3:

- The overall accuracy level is only 51%; certainly not great.  The precision and recall varies across

the different 5 categories.  The F1 scores which is the Harmonic mean is more discerning, with 

Diagnostic Radiology & Cardiology better than 50%.  However, its still not great and the next 

question is what can we do to improve it?

--------------------------------------------------------------------------------

\clearpage

## Can we perform some statistical procedures to better understand the predictors and improve the model based on these statistical findings?

--------------------------------------------------------------------------------

\clearpage


## Gaussian Normality checks across predictors 

(A) - One of Bayes assumption in case of numeric features is that the predictors should be normally 

distributed.

- Let's check to see if our predictors have normal distribution, and could that be our problem?

## Correlation checks across all predictors 

(B) - The 2nd premise of the Naive Bayes is that it assumes predictors to be independent.

This assumption is hardly the case in reality, so in order to use Naive Bayes, lets review some of the

the statistical relationships amongst the predictors itself, again could that be our problem as well?

--------------------------------------------------------------------------------

\clearpage

## Part A:  Gaussian check on predictors; Using Shapiro Wilks test

```{r}

# Make sure no zeros resides in Procedure types; HCPCS codes column
data_temp <- data5%>% 
  mutate_if(sapply(data5, is.character),as.factor) %>% 
  select(NPI,Provider_Type, HCPCS_Code,No_Services) %>% 
  filter(No_Services != 0 ) %>%
  mutate_at(vars(Provider_Type),as.character)

#Normality test: using Shapiro test.  This to extract only normal prectors 

tmp1 <- dcast(as.data.table(data_temp ), Provider_Type~HCPCS_Code, value.var="No_Services", fun.aggregate=sum, fill=0) %>%
  as_tibble() %>% 
  mutate_at(vars(-Provider_Type), as.numeric)

tmp2 <- tmp1 %>% select(-c(Provider_Type))
# apply shapiro test on all the predictors 
lshap <- lapply(tmp2, shapiro.test)
#lshap[[1]]#just to take a peek

lres <- sapply(lshap,`[`, c("statistic","p.value"))

#head(lres ,n=2)

mat <- t(lres) #create a Matrix of Statistics and P.values columns

not_normal_predictors <- mat[(mat[,2]<0.05),] # p-values < 0.05 are not normal predictors
normal_predictors <- mat[(mat[,2]>0.05),] # p-values > 0.05 are normal predictors

#count the no. of normal vs. non-normal predictors
total_non_normal <- nrow(not_normal_predictors)
total_normal <- nrow(normal_predictors)

#keeping normal predictors only; this is the dataframe with only normal predictors 
x_onehot4 <- dcast(as.data.table(data_temp ), NPI+Provider_Type~HCPCS_Code, value.var="No_Services", fun.aggregate=sum, fill=0) %>%
  as_tibble() %>% 
  mutate_at(vars(-NPI,-Provider_Type), as.numeric)

proc_to_keep <- as.character(rownames(normal_predictors))
x_onehot4a <- x_onehot4 %>%
  select(NPI, Provider_Type, proc_to_keep)

# no. of normal vs. non-normal predictors found
print(paste0('Total non normal predictors : ', total_non_normal))

print(paste0('Total normal predictors : ', total_normal))

```

--------------------------------------------------------------------------------

\clearpage

Observation 3a:

- From Shapiro testing, 1511 of the subsetted predictors were not normal while 342 were. This means

that only 18% of the predictors were Gaussian.  It maybe because a majority of the predictors were non

normal, the Naive Bayes model did not perform as well as it could have

--------------------------------------------------------------------------------

\clearpage

## Part B:  Correlational Study across all predictor variables

```{r}
dat_corr <- x_fl_widerfinal %>% 
  select(-c(Provider_Type)) #removing the provider types for simplicity

head(dat_corr,n=2)
#unfactor to convert it to numeric in order to perform correlation
y <- unfactor(dat_corr) #another temp object to contain just the predictors

#build the correlation matrix
econCor <- cor(y[,])
# melt it into long format
econMelt <- melt(econCor, varnames = c("X","Y"),value.name ="Correlation")

# Order it according to the correlation
econMelt <- econMelt[order(econMelt$Correlation),]
#Display the melted data; correlation table

econMelt_min <- econMelt %>% 
    #filter(abs_Correlation>0.01|abs_Correlation<(-0.01)) # virtually zero correlated predictors 
    filter(Correlation<=0.01&Correlation>=(-0.01)) # virtually zero correlated predictors 

# To check and extract only the  (distinct) ubcorrelated predictors
check <- econMelt_min %>% 
  distinct(Y)
head(check, n=2)
```

## Some Visualizations on the selected predictors

Observation 3b:

- The Top 5 Floridian datasets had 1,407 distinct predictors as stored in dat_corr object

- In filtering out the predictor correlations between -0.01 to 0.01, we still end up with 1,407 of 

the same distinctive predictors;  this tells me that the predictors are already low in correlation

and therefore not the cause of our low accuracy that we've seen thus far

- See The Heat Map w/o labels below (so we can actually visualize it better); They all the same color!

## Predictor correlations was not contributing to low accuracy levels in the model, see below

```{r}

# Correlational relationships between the predictors

# Heat mapping the predictors
hm.palette <- colorRampPalette(rev(brewer.pal(9, 'YlOrRd')), space='Lab')
ggplot(econMelt_min, aes(x = X, y = Y)) + ggtitle("Fig 4: Heat Map across all Predictors; X & Y represent all the Predictors" )+
  geom_tile(aes( fill = Correlation)) +theme_minimal()+
  coord_equal() + theme(axis.text.x=element_blank())+theme(axis.text.y=element_blank())+
  scale_fill_gradientn(colours = hm.palette(100),
                       guide=guide_colorbar(ticks=FALSE, barheight = 10),
                       limits=c(-0.05,0.05))

```

##  Lets try Naive Bayes again, but this time only using the normal predictors we have found

- This is to see if using only the normal predictors can improve the Naive Bayes model?

- The smaller, normal only predictors now reside in x_onehot4a dataframe

```{r}
indxTrainnormal <- createDataPartition(x_onehot4a$Provider_Type,p = 0.70,list = FALSE)
trainingnormal <- x_onehot4a[indxTrainnormal,]
testingnormal <- x_onehot4a[-indxTrainnormal,]

xnorm = trainingnormal[,-1]
ynorm = as.factor(trainingnormal$Provider_Type)


model_normal = train(xnorm,ynorm,'nb',trControl=trainControl(method='cv',number=5),laplace = 0.1,na.action = na.pass)

 model_normal
```

## Confusision Matrix: 2nd try with Naive Bayes based on only the 342 normal predictors

```{r}
#Predict testing set; with normal predictor second time
Predict_normal <- predict( model_normal,newdata = testingnormal)

table(Predict_normal, testingnormal$Provider_Type,dnn=c("Prediction","Actual"))

#Get the confusion matrix to see accuracy value and other parameter values
 
cm_normal <- confusionMatrix(data=Predict, reference=as.factor(testing$Provider_Type))

cm_normal
```

## F1 scores : 2nd try with Naive Bayes based only on the 342 normal predictors

```{r}
# extract F1 scores for all classes second time
mylist_normal <- cm_normal[["byClass"]][,"F1"]

mylist_normal <- tibble(mylist_normal)

my_F1_score_normal <- mylist_normal %>% 
  mutate(byClass= byClass, F1_Score = as.factor(percent(round(mylist_normal,digits=2)))) %>% select(-c(mylist_normal)) %>% arrange(desc(F1_Score ))

my_F1_score_normal 

```

--------------------------------------------------------------------------------

\clearpage

Observation 3c:

- In the Statistical dignostics: checking for normality and correlation of the predictors revealed

that even in using uncorrelated predictors that were normal, the overall accuracy level did not improve

nor help with the F1 scores in the Naive Bayes Model.

--------------------------------------------------------------------------------

\clearpage

## Try RandomForest with only the normal predictors from above

- Can Random Forest help with accuracy levels?

---

## NOTE: Random Forest is a less restrictive model

- It does not require predictors to behave normally

- It does not require predictors to be uncorrelated with each other

---

- HOwever, we kept the same 342 normal predictors to keep the sample size equal in terms of number of

predictors and more importantly to maintain an apples-to-apples comparision between Naive Bayes and

 Random Forest

```{r}
set.seed(150)

indxTrain1 <- createDataPartition(x_onehot4a$Provider_Type,p = 0.65,list = FALSE)
training1 <- x_onehot4a[indxTrain1,]
testing1 <- x_onehot4a[-indxTrain1,]

# Training using ‘random forest’ algorithm with only 10 trees due to limited computer capacity

training1$Provider_Type <- as.factor(training1$Provider_Type)
model1 <- train(Provider_Type ~ ., data=training1[,-1],
                method = 'rf',ntree=10,trControl = trainControl(method = 'cv', number = 5) )

model1
#caret::confusionMatrix(model1)
```

## Predicting with Random Forest

```{r}
#Predict testing set
Predict1 <- predict(model1,newdata = testing1[,-1] )

testing2 <- testing1 %>%
  mutate(Predicted=Predict1,
         Provider_Type=as.factor(Provider_Type)) %>%
  select(NPI, Provider_Type, Predicted, everything())

#Confusion Matrix - Random Forest
cm_rf <- confusionMatrix(data=testing2$Provider_Type, reference=testing2$Predicted)
cm_rf

```

--------------------------------------------------------------------------------

\clearpage

Observation 3d:

- The Overall Accuracy level went up to 64% and all the F1 scores across the 5 classes increased too

-  Note:  Only 10 trees were utilized due to size constraint, See Figure 5.

--------------------------------------------------------------------------------

\clearpage

##F1 scores - Random Forest

```{r}
#F1 scores - Random Forest
f1 <- cm_rf[["byClass"]][,"F1"]

mylist_rf <- tibble(f1)

my_F1_score_rf <- mylist_rf  %>% 
  mutate(byClass= byClass, F1_Score = as.factor(percent(round(f1,digits=2)))) %>% select(-c(f1 )) %>% arrange(desc(F1_Score))

my_F1_score_rf 

```

## Biggest contributors to the predicting the 5 classes:  Important variables

The 5 classes are:

- 'Cardiology','Diagnostic Radiology','Family Practice','Internal Medicine','Nurse Practitioner'

```{r}
#Importance Variables to the classes & Plot Variable performance
X_rf <- varImp(model1)
#Plot only top 20 that contributes most to predicting the classes (responses)
plot(X_rf, top=20)

```

--------------------------------------------------------------------------------

\clearpage

Observation 3e:

Importance Variable Contribution:

- The plot sums up how each predictor variable is independently responsible for predicting the

outcome.

--------------------------------------------------------------------------------

\clearpage

## Test the effectiveness of the exclusion table

- Compare side-by-side the actuals (provider type) vs. the predicted from test data set

```{r}
#joins test results with exclustion datatable;if we can catch the "bad doctor" with fraudelent past
testing3 <- testing2 %>%
  left_join(df1 %>% select(NPI, EXCLTYPE), by="NPI") %>%
  select(NPI, Provider_Type,Predicted,EXCLTYPE,everything())

testing3[!is.na(testing3$EXCLTYPE),]

match_df1 <- testing3 %>% 
  select(NPI, Provider_Type, Predicted,EXCLTYPE ) %>% 
  filter(EXCLTYPE != "NA")

exclusion_check_df <- inner_join(df1, match_df1 , by =c("EXCLTYPE",'NPI')) %>% 
  select(GENERAL,SPECIALTY,NPI,Provider_Type,Predicted,STATE,EXCLTYPE,EXCLDATE,REINDATE)

exclusion_check_df 
```

--------------------------------------------------------------------------------

\clearpage

Observation 4a:

- The final output shows that we built a Random Forest classifier that can predict whether physician's

actual provider type is different from its predicted provider type, with an accuracy of around 64%.

- Based on this small data set, 2 doctors with NPIs 1023077047 & 1477553840 respectively were found 

in OIG's exclusion table.

- With an overall 64% accuracy level, 1477553840 which is a Family Practice physician was predicted to be

Internal Medicine while 1023077047 which is a Cardiolgist is also predicted to be an Internal Medicine

specialist.  This differences is what the tool will throw out; this could be a flag for further

investigation. 

--------------------------------------------------------------------------------

\clearpage

## Visual Comparision of accuracy between Naive Bayes to Random Forest 

- Side by side plots of F1 scores 
 
```{r,fig.width=12, fig.height=7}

#merging the 3 dataframes of F1 scores from the Naive Bayes to Random Forest Runs
my_F1_score1<-my_F1_score %>% mutate(Model = "NB all predictors")
my_F1_score_normal1<-my_F1_score_normal %>% mutate(Model = "NB Normal only predictors")
my_F1_score_rf1<-my_F1_score_rf %>% mutate(Model = "RF Normal only predictors")

F1_final_joins <- rbind(my_F1_score1,my_F1_score_normal1,my_F1_score_rf1   )

# arrange multiple plots in one page

F1_final_joins  %>% 
  ggplot(aes(x=Model, y=F1_Score, fill = byClass)) +
  geom_bar(stat="identity", width = 0.5,  position="dodge")+

  ggtitle("Fig5: F1 Scores Comparision ")+
  theme_economist()

```

--------------------------------------------------------------------------------

\clearpage

Observation 4b: Figure 5 Visual representation

- F1 Scores did not improve across all classes when going from all predictors to only normal

predictors in the Naive Bayes Model 

- F1 Scores exhibited a significant improvements across all classes in the Random Forest Model

This suggest Random Forest is a more superior model than Naive Bayes, ceteris paribus.

--------------------------------------------------------------------------------

\clearpage

## hypothesis testing: Chi-Square Test of Independence

```{r}
chisq.test(x=testing3$Provider_Type, y=testing3$Predicted)

```

--------------------------------------------------------------------------------

\clearpage

Observation 4b:

Ho:  there is no difference between actual and predicted provider types

Ha:  there is a difference (not equal to) between actual and predicted provider types

- The p-value is essentially zero, therefore we reject the Ho.  Again, this should be taken with a big

caveat, see data requirements below 

## Data Requirement (before we get carried out with ChiSquare's conclusion) [1]


1) Independence of observations.

2) There is no relationship between the subjects in each group.

3) The categorical variables are not "paired" in any way (e.g. pre-test/post-test observations).

4) Relatively large sample size. 

5) Expected frequencies for each cell are at least 1.

6) Expected frequencies should be at least 5 for the majority (80%) of the cells.

Given my need to down sample my data to make this worked, I am quite sure one or more of the above 

conditions were violated. The predictive power of the model is not as strong as it could've been only, 

64% overall accuracy with 10 trees.  This resulted in differences between actuals and predictions for

sure. So The hypothesis testing conclusion should be taken with a grain of salt!

--------------------------------------------------------------------------------

\clearpage

## Summary & Conclusions

This tool provide the potential to give some insight in determining if, and when, physicians are acting

outside the norm of their respective specialty, which could indicate misuse, fraud, or lack of knowledge

around billing procedures [2]. A publicly available procedure billing dataset, released by the U.S.

Medicare system was the main data source. Due to the large size of the dataset, I sampled the dataset to

include only physicians practicing within one state; Florida. The model uses the multinomial Naïve

Bayes and Random Forest by evaluating the F1-Scores with 5-fold cross-validation. The model was

able to successfully predict several classes of physicians with an F-score over 90%. These results show

that it is possible to effectively use machine learning in a novel way to classify physicians into their

respective ﬁelds solely using the procedures they bill for. This tool provides a model that can identify

physicians who are potentially misusing insurance systems for further investigation.

Florida was a good candidate for testing this method for several reasons because it included the second

highest number of Medicare beneﬁciaries and second in total Medicare spending [2].  However, due to

computation limitations, I had to further reduced the size to only top 5 Floridian provider types;

'Cardiology','Diagnostic Radiology','Family Practice','Internal Medicine','Nurse Practitioner'.  

This made the datasize more digestable. Naive Bayes and Random Forest models were then applied in the

study.  Statistical diagnostics were used to determine the validity of the two main assumptions in the

Naive Bayes algorithm; normality and uncorrelation of the predictors.  First, it was found that 

only 18% of the predictors used were normally distributed as required by Bayesian. Second, it was also

found that all the predictors were not correlated or have very low correlations with each other as

required by Bayesian as well.  Given these findings, only Gaussian predictors (342) were used to improve

the accuracy levels of the Naive Bayes algorithm.  However, the 2nd run Naive Bayes run did not

experienced any improvements at all even after applying only the normal predictors.

Random Forest algorithm were then applied to the same 342 predictors and it was found to be have

significantly improved the accuracy of the model; all the F1 scores saw improvements.  

It was clear from the results that Random Forest was the superior of the two algorithms; all the F1

scores across the 5 classes improved significantly.

Finally, this is where the tool could help; when the predicted classes were matched up against the

Inspector's General exclusion data table, 2 doctors had anomalous predictions vs.its actual provider

type.  These 2 physicians who are a family practice and cardiologist respectively were both predicted to

be internal medicine and they were also found to be in the exclusion list.  This meant they both had a

fraudelent past and had been flagged by the State of Florida and this tool confirms that was the case.

## NOTE: Professor Tati, you may get a different prediction provider type when you run this program.  

Areas of Improvement:

-  Because many different types of physicians can perform the same procedures, future models should 

account for this procedure overlap, as this does not represent an anomaly or fraud, but merely regular

doctors practicing everyday medecine

-  The most likely reason why a simple learner model like Naive Bayes were not able to discern the 

classes each provider types falls into was the lack of a standard dataset for insurance fraud. Therefore,

there needs to be a set of rules and regulations to established a baseline behavior for physicians in

terms of insurance utilization for each specialty

-  Finally, obviously this is a simple model with a small filtered dataset due to computer constraints, 

future work should expand on a bigger sample size before we can have full confidence on the accuracies

of the results; especially the conclusion of the hypothesis testing.

References:

(1) https://libguides.library.kent.edu/SPSS/ChiSquare

(2) R. Bauder, T. M. Khoshgoftaar, Aaron Ritcher and Mathew Herland, “Predicting Medical Provider

Specialties to Detect Anomalous Insurance Claims”, Florida Atlantic University, 2016 IEEE 28th

International Conference on Tools with Artificial Intelligence. 

(3) T. H. J. K. F. Foundation, “State Health Facts - Medicare,” 2015. [Online]. Available: http://kff.org/state-category/medicare/
  
--------------------------------------------------------------------------------

\clearpage



